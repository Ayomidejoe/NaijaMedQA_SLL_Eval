{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLPKNXo1CAhV6vUtq5AqQt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayomidejoe/NaijaMedQA_SLL_Eval/blob/main/NaijaMedQASLLeval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies for Gemini and Hugging Face quantization\n",
        "!pip install -q google-generativeai nest_asyncio\n",
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install -q sentence-transformers evaluate rouge_score\n",
        "!pip install -q evaluate # Ensure evaluate is installed\n",
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "Zr_pDMHtMDO8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import gc\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import google.generativeai as genai\n",
        "from google.api_core import exceptions\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import google.auth\n",
        "import gspread\n",
        "import gspread_dataframe\n",
        "from google.auth import default as get_default_credentials"
      ],
      "metadata": {
        "id": "YIrNFhgZ1WHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.auth\n",
        "from google.colab import auth\n",
        "# 1. Authenticate the user\n",
        "auth.authenticate_user()\n",
        "\n",
        "# 2. Get the credentials\n",
        "creds, _ = google.auth.default()\n",
        "\n",
        "# 3. Authorize gspread\n",
        "gs = gspread.authorize(creds)\n",
        "\n",
        "# 4. Open the sheet\n",
        "spreadsheet_url = \"https://docs.google.com/spreadsheets/d/13AsHGT68HfClFbPXLJNjrXSTtq7UggnqkXKgV9YC78w\"\n",
        "wks = gs.open_by_url(spreadsheet_url).worksheet('Sheet1')\n",
        "\n",
        "# 5. Correct function name: get_as_dataframe\n",
        "df_gsheet = gspread_dataframe.get_as_dataframe(wks)\n",
        "\n",
        "# 6. Select columns and clean up empty rows/columns (common with gspread_dataframe)\n",
        "df_test_cases = df_gsheet[['Question', 'Doctor_answer']].head(5)\n",
        "\n",
        "print(\"Successfully loaded data:\")\n",
        "print(df_test_cases)"
      ],
      "metadata": {
        "id": "mwYYoO8X1-wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Async Patch (if not already applied)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Setup Google Gemini (The Judge)\n",
        "# Ensure your GOOGLE_API_KEY is set in the environment or directly here\n",
        "# For Colab, it's recommended to store it as a Colab secret\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"APIKEY\"\n",
        "genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"), transport=\"rest\")\n",
        "\n",
        "# Re-defining model_ids and bnb_config\n",
        "model_ids = [\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    \"microsoft/phi-3.5-mini-instruct\",\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    \"stabilityai/stablelm-zephyr-3b\"\n",
        "\n",
        "]\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Helper function to clear GPU memory\n",
        "def cleanup():\n",
        "    # Using 'global' ensures we are pointing to the models defined outside\n",
        "    global model, tokenizer\n",
        "\n",
        "    if 'model' in globals(): del model\n",
        "    if 'tokenizer' in globals(): del tokenizer\n",
        "\n",
        "    import gc # Import here specifically to be safe\n",
        "    gc.collect()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        torch.cuda.ipc_collect()\n",
        "\n",
        "    print(\"GPU Memory Cleared.\")"
      ],
      "metadata": {
        "id": "IKfZ57D713cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Structure ---\n",
        "class LLMTestCase:\n",
        "    def __init__(self, input_query: str, reference_answer: str):\n",
        "        self.input = input_query\n",
        "        self.reference = reference_answer  # The \"Gold Standard\"\n",
        "        self.actual_output = None\n",
        "        self.scores = {} # Dictionary to store multiple metrics\n",
        "        self.reasoning = None\n",
        "\n",
        "# --- The \"Evaluatee\" (Lightweight Model Wrapper) ---\n",
        "class MedicalLLMClient:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def generate_answer(self, test_case: LLMTestCase):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a specialized medical assistant. Answer the user's question. Be highly concise and professional.\"},\n",
        "            {\"role\": \"user\", \"content\": test_case.input}\n",
        "        ]\n",
        "\n",
        "        text = self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        inputs = self.tokenizer([text], return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=150,\n",
        "                temperature=0.3 # Keep it factual\n",
        "            )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        if \"assistant\" in response:\n",
        "            return response.split(\"assistant\")[-1].strip()\n",
        "        return response.split(\"\\n\")[-1].strip()\n",
        "\n",
        "# --- The \"Judge\" (Gemini Wrapper) ---\n",
        "class GeminiRelevancyMetric:\n",
        "    def __init__(self):\n",
        "        self.judge_model = genai.GenerativeModel(\n",
        "            model_name='gemini-2.5-flash'\n",
        "        )\n",
        "\n",
        "    def evaluate_with_retry(self, prompt, retries=3, delay=2):\n",
        "        for i in range(retries):\n",
        "            try:\n",
        "                response = self.judge_model.generate_content(\n",
        "                    prompt,\n",
        "                    generation_config={\"response_mime_type\": \"application/json\"}\n",
        "                )\n",
        "                return response.text\n",
        "            except (exceptions.InternalServerError, exceptions.ServiceUnavailable, Exception) as e:\n",
        "                if i == retries - 1: raise e\n",
        "                print(f\"   Connection glitch, retrying in {delay}s... ({str(e)[:50]})\")\n",
        "                time.sleep(delay)\n",
        "                delay *= 2\n",
        "\n",
        "    async def evaluate(self, test_case: LLMTestCase):\n",
        "        evaluation_prompt = f\"\"\"\n",
        "        You are an expert medical evaluator.\n",
        "        rate the answer given and give score with the highest being 1.\n",
        "\n",
        "        Question: {test_case.input}\n",
        "        Actual Output: {test_case.actual_output}\n",
        "\n",
        "        Return JSON: {{\\\"score\\\": float, \\\"reason\\\": string}}\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            loop = asyncio.get_event_loop()\n",
        "            raw_text = await loop.run_in_executor(None, self.evaluate_with_retry, evaluation_prompt)\n",
        "\n",
        "            clean_json = raw_text.replace('```json', '').replace('```', '').strip()\n",
        "            result = json.loads(clean_json)\n",
        "\n",
        "            test_case.score = result.get(\"score\", 0)\n",
        "            test_case.reasoning = result.get(\"reason\", \"No reason provided\")\n",
        "        except Exception as e:\n",
        "            test_case.score = 0\n",
        "            test_case.reasoning = f\"Evaluation Failed: {str(e)}\"\n",
        "\n",
        "        return test_case.score\n",
        "\n",
        "# --- Advanced Evaluator ---\n",
        "class AdvancedEvaluator:\n",
        "    def __init__(self):\n",
        "        self.sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.rouge = evaluate.load('rouge')\n",
        "\n",
        "    def calculate_metrics(self, test_case: LLMTestCase):\n",
        "        ref = test_case.reference\n",
        "        hyp = test_case.actual_output\n",
        "\n",
        "        emb1 = self.sbert_model.encode(ref, convert_to_tensor=True)\n",
        "        emb2 = self.sbert_model.encode(hyp, convert_to_tensor=True)\n",
        "        sbert_score = util.cos_sim(emb1, emb2).item()\n",
        "\n",
        "        rouge_results = self.rouge.compute(\n",
        "            predictions=[hyp],\n",
        "            references=[ref],\n",
        "            use_aggregator=False\n",
        "        )\n",
        "\n",
        "        test_case.scores = {\n",
        "            \"SBERT_Similarity\": round(sbert_score, 4),\n",
        "            \"F1_Score\": round(rouge_results['rougeL'][0], 4),\n",
        "            \"Recall\": round(rouge_results['rouge1'][0], 4),\n",
        "            \"Precision\": round(rouge_results['rouge2'][0], 4)\n",
        "        }\n",
        "\n",
        "# 1. Define Medical Test Cases from Google Sheet\n",
        "def create_medical_test_cases(df):\n",
        "    medical_test_cases = []\n",
        "    for _, row in df.iterrows():\n",
        "        medical_test_cases.append(\n",
        "            LLMTestCase(\n",
        "                input_query=row['Question'],\n",
        "                reference_answer=row['Doctor_answer']\n",
        "            )\n",
        "        )\n",
        "    return medical_test_cases"
      ],
      "metadata": {
        "id": "7hlf748J4fb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Generate and Evaluate ---\n",
        "async def run_full_pipeline(model, tokenizer, test_cases):\n",
        "    print(f\"--- Processing {len(test_cases)} Medical Queries ---\")\n",
        "\n",
        "    med_client = MedicalLLMClient(model, tokenizer)\n",
        "    gemini_judge = GeminiRelevancyMetric()\n",
        "    adv_evaluator = AdvancedEvaluator()\n",
        "\n",
        "    for i, case in enumerate(test_cases):\n",
        "        print(f\"\\n[{i+1}/{len(test_cases)}] Question: {case.input}\")\n",
        "        case.actual_output = med_client.generate_answer(case)\n",
        "        print(f\"   Response: {case.actual_output[:100]}...\")\n",
        "\n",
        "        print(\"   Calculating SBERT & Overlap scores...\")\n",
        "        adv_evaluator.calculate_metrics(case)\n",
        "\n",
        "        print(\"   Requesting Gemini Judge Evaluation...\")\n",
        "        await gemini_judge.evaluate(case)\n",
        "        print(f\"   Score: {case.score} | Reason: {case.reasoning[:60]}...\")\n",
        "\n",
        "        if i < len(test_cases) - 1:\n",
        "            print(\"   Sleeping 6s to avoid API Quota (429) errors...\")\n",
        "            await asyncio.sleep(6)\n",
        "\n",
        "    print(\"\\nAll evaluations complete for this model.\")\n",
        "\n",
        "    current_model_results = []\n",
        "    for c in test_cases:\n",
        "        res = {\n",
        "            \"Question\": c.input,\n",
        "            \"Judge Score\": c.score,\n",
        "            \"SBERT\": c.scores.get(\"SBERT_Similarity\"),\n",
        "            \"F1\": c.scores.get(\"F1_Score\"),\n",
        "            \"Recall\": c.scores.get(\"Recall\"),\n",
        "            \"Judge Reason\": c.reasoning\n",
        "        }\n",
        "        current_model_results.append(res)\n",
        "    return current_model_results\n",
        "\n",
        "\n",
        "# --- The Comparison Loop ---\n",
        "all_model_results_list = []\n",
        "\n",
        "for m_id in model_ids:\n",
        "    print(f\"\\nNOW LOADING: {m_id}\")\n",
        "\n",
        "    # Use float16 or 4-bit quantization to save RAM\n",
        "    tokenizer = AutoTokenizer.from_pretrained(m_id)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        m_id,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        low_cpu_mem_usage=True  # Important to prevent RAM spikes\n",
        "    )\n",
        "\n",
        "    dataset = create_medical_test_cases(df_test_cases)\n",
        "    model_evaluation_results = await run_full_pipeline(model, tokenizer, dataset)\n",
        "\n",
        "    for res in model_evaluation_results:\n",
        "        res[\"Model_ID\"] = m_id\n",
        "        all_model_results_list.append(res)\n",
        "\n",
        "    # --- AGGRESSIVE CLEANUP ---\n",
        "    del model\n",
        "    del tokenizer\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# 4. Display Results in a Table\n",
        "pd.set_option('display.max_colwidth', 150)\n",
        "df_display = pd.DataFrame(all_model_results_list)\n",
        "display(df_display)\n",
        "\n",
        "# Make the combined results available for plotting\n",
        "global results_df\n",
        "results_df = df_display # This will be used by the plotting cell"
      ],
      "metadata": {
        "id": "lVx_056K5AzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78088c2c"
      },
      "source": [
        "#Calculate Average Scores and Rank Models\n",
        "df_model_avg_scores = results_df.groupby('Model_ID')[['Judge Score', 'SBERT', 'F1', 'Recall']].mean().reset_index()\n",
        "df_model_avg_scores = df_model_avg_scores.sort_values(by='Judge Score', ascending=False)\n",
        "print(\"Average Scores and Model Ranks:\")\n",
        "display(df_model_avg_scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ea05f8c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Bar chart for Average Judge Score\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='Model_ID', y='Judge Score', data=df_model_avg_scores, palette='viridis', hue='Model_ID', legend=False)\n",
        "plt.title('Average Judge Score per Model')\n",
        "plt.xlabel('Model ID')\n",
        "plt.ylabel('Average Judge Score')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Grouped bar chart for SBERT, F1, and Recall\n",
        "df_melted_scores = df_model_avg_scores.melt(id_vars=['Model_ID'], value_vars=['SBERT', 'F1', 'Recall'],\n",
        "                                            var_name='Metric', value_name='Score')\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Model_ID', y='Score', hue='Metric', data=df_melted_scores, palette='tab10')\n",
        "plt.title('Comparison of Average SBERT, F1, and Recall Scores per Model')\n",
        "plt.xlabel('Model ID')\n",
        "plt.ylabel('Average Score')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title='Metric')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}